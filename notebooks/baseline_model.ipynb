{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "---\n",
    "\n",
    "The aim of this script is to provide a baseline model for our electricity prices project.\n",
    "\n",
    "Idea:\n",
    "For the baseline model, we assume that the predicted value for each hour is the same as the value from the previous day at the same hour. This simple approach leverages the daily seasonal pattern often observed in electricity prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MLFlow -> probably not possible as no ML is used???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MLFlow package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Setup #######################################\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# Load login data from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USERNAME = os.getenv('DB_USERNAME')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "\n",
    "DB_STRING = f'postgresql://{DB_USERNAME}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(DB_STRING)\n",
    "\n",
    "# Create a new connection using psycopg2 for non-pandas operations\n",
    "conn = psycopg2.connect(\n",
    "    database=DB_NAME,\n",
    "    user=DB_USERNAME,\n",
    "    password=DB_PASSWORD,\n",
    "    host=DB_HOST,\n",
    "    port=DB_PORT\n",
    ")\n",
    "\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"You are connected to -\", record, \"\\n\")\n",
    "    \n",
    "    ####################################### EXTRACT #######################################\n",
    "    \n",
    "    # Load data from the database using SQLAlchemy engine\n",
    "    print(\"Energy generation data loading!\")\n",
    "    query_string2 = 'SELECT * FROM \"03_gold\".\"FILENAME_HERE\"'\n",
    "    df_baseline = pd.read_sql(query_string2, engine)\n",
    "    print(\"Loading finished!\")\n",
    "\n",
    "except Exception as error:\n",
    "print(\"Error while connecting to PostgreSQL:\", error)\n",
    "    \n",
    "finally:\n",
    "    if conn:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag feature for the same hour on the previous day\n",
    "df_baseline['price_lag24'] = df_baseline['price'].shift(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values introduced due to the lag\n",
    "df_baseline.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target and the feature (lagged values)\n",
    "X = df_baseline.drop(columns=['price'])\n",
    "y = df_baseline['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of splits\n",
    "n_splits = 5\n",
    "test_size = int(len(X) / (n_splits + 1))\n",
    "\n",
    "splits = []\n",
    "mse_scores = []\n",
    "\n",
    "for i in range(n_splits):\n",
    "    train_end = (i + 1) * test_size\n",
    "    test_end = train_end + test_size\n",
    "    \n",
    "    # Ensure that train_end does not go beyond the length of the dataset\n",
    "    if test_end > len(X):\n",
    "        test_end = len(X)\n",
    "\n",
    "    X_train = X.iloc[:train_end]\n",
    "    X_test = X.iloc[train_end:test_end]\n",
    "    y_train = y.iloc[:train_end]\n",
    "    y_test = y.iloc[train_end:test_end]\n",
    "    \n",
    "    splits.append((X_train, X_test, y_train, y_test))\n",
    "    \n",
    "    # Predict using baseline (prior day's values)\n",
    "    y_pred = X_test['lagged_target']\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test.index, y_test, label='Actual', color='blue')\n",
    "    plt.plot(y_test.index, y_pred, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'Actual vs Predicted for Split {i+1}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Target')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Split {i+1}: Training data: {X_train.shape}, Validation data: {X_test.shape}\")\n",
    "    print(f\"Training target: {y_train.shape}, Validation target: {y_test.shape}\")\n",
    "    print(f\"MSE for Split {i+1}: {mse}\\n\")\n",
    "\n",
    "# Optional: Average MSE across all splits for overall evaluation\n",
    "average_mse = sum(mse_scores) / len(mse_scores)\n",
    "print(f\"Average MSE across all splits: {average_mse}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
